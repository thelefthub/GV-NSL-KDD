{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82f2e0cf",
   "metadata": {},
   "source": [
    "## 1 - Data preparation\n",
    "\n",
    "As proposed by Tavallaee et al. (2009) a cleaned and improved version of the original KDD Cup 1999 dataset will be used. It contains no redundant records, balanced difficulty levels, and better representative samples. It should be ready for direct use in ML pipelines after encoding categorical variables and normalization/scaling of numeric ones. The difficulty_level column was added by Tavallaee et al. (2009) when they created the NSL-KDD dataset to address weaknesses in the original KDD Cup 1999 data. This meta label will not be used in the project.\n",
    "\n",
    "The preprocessing step performs several key transformations to prepare the NSL-KDD dataset for its intended purpose. First, categorical features such as protocol_type, service, and flag are encoded into numeric form using LabelEncoder, ensuring that the model can process non-numerical data. The target labels (i.e. attack types) are also converted from text strings to integer values for classification. Next, all numeric features are standardized with StandardScaler, which centers the data around a mean of zero and a standard deviation of one. Scaling is fitted only on the training data to prevent data leakage, and the same transformation is then applied to the validation and test sets. The different steps ensure that the models receive data in a consistent numerical format where no single feature dominates due to differing scales.\n",
    "\n",
    "Finally, the fitted encoders and scaler could be saved using joblib so that they can be reloaded later without refitting, ensuring reproducibility when the model is reused. However, because this project is a demo course project, the code ignores some best practices such as saving transformer objects, implementing detailed exception handling, validating file paths, and using configuration files for data directories. In a full production pipeline, these additions would be essential to ensure reproducibility, maintainability and robustness of the preprocessing workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f996285c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded labels:\n",
      "0 : apache2\n",
      "1 : back\n",
      "2 : buffer_overflow\n",
      "3 : ftp_write\n",
      "4 : guess_passwd\n",
      "5 : httptunnel\n",
      "6 : imap\n",
      "7 : ipsweep\n",
      "8 : land\n",
      "9 : loadmodule\n",
      "10 : mailbomb\n",
      "11 : mscan\n",
      "12 : multihop\n",
      "13 : named\n",
      "14 : neptune\n",
      "15 : nmap\n",
      "16 : normal\n",
      "17 : perl\n",
      "18 : phf\n",
      "19 : pod\n",
      "20 : portsweep\n",
      "21 : processtable\n",
      "22 : ps\n",
      "23 : rootkit\n",
      "24 : saint\n",
      "25 : satan\n",
      "26 : sendmail\n",
      "27 : smurf\n",
      "28 : snmpgetattack\n",
      "29 : snmpguess\n",
      "30 : spy\n",
      "31 : sqlattack\n",
      "32 : teardrop\n",
      "33 : udpstorm\n",
      "34 : warezclient\n",
      "35 : warezmaster\n",
      "36 : worm\n",
      "37 : xlock\n",
      "38 : xsnoop\n",
      "39 : xterm\n",
      "Finished processing data...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "OUT_DIR  = \"data/processed\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Created using NSL-KDD documentation\n",
    "columns = [\n",
    "    \"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\"dst_bytes\",\n",
    "    \"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\"logged_in\",\n",
    "    \"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\"num_file_creations\",\n",
    "    \"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\"is_host_login\",\"is_guest_login\",\n",
    "    \"count\",\"srv_count\",\"serror_rate\",\"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\n",
    "    \"same_srv_rate\",\"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\",\"difficulty_level\"\n",
    "]\n",
    "\n",
    "categorical_cols = [\"protocol_type\", \"service\", \"flag\"]\n",
    "\n",
    "train_df = pd.read_csv(f\"{DATA_DIR}/KDDTrain+.txt\", names=columns)\n",
    "test_df  = pd.read_csv(f\"{DATA_DIR}/KDDTest+.txt\",  names=columns)\n",
    "\n",
    "# drop the meta label\n",
    "train_df.drop(columns=[\"difficulty_level\"], inplace=True, errors=\"ignore\")\n",
    "test_df.drop(columns=[\"difficulty_level\"],  inplace=True, errors=\"ignore\")\n",
    "\n",
    "# just to make sure all labels are known before using test set (i.e. to prevent unseen-category errors)\n",
    "labels = {}\n",
    "for col in categorical_cols:\n",
    "    enc = LabelEncoder()\n",
    "    enc.fit(pd.concat([train_df[col], test_df[col]], axis=0))\n",
    "    train_df[col] = enc.transform(train_df[col])\n",
    "    test_df[col]  = enc.transform(test_df[col])\n",
    "    labels[col] = enc\n",
    "\n",
    "# print(labels)\n",
    "X_train_full = train_df.drop(columns=[\"label\"])\n",
    "y_train_full = train_df[\"label\"]\n",
    "X_test = test_df.drop(columns=[\"label\"])\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "\n",
    "# encode labels into integers\n",
    "y_encoder = LabelEncoder().fit(pd.concat([y_train_full, y_test], axis=0))\n",
    "y_train_full_enc = y_encoder.transform(y_train_full)\n",
    "y_test_enc = y_encoder.transform(y_test)\n",
    "\n",
    "print(\"Encoded labels:\")\n",
    "for i, label in enumerate(y_encoder.classes_):\n",
    "    print(i, \":\", label)\n",
    "\n",
    "# perform scaling (standardization) for training data (cf. src_bytes vs error_rate)\n",
    "# prevent data leakage by excluding test in scaler\n",
    "scaler = StandardScaler().fit(X_train_full)\n",
    "X_train_full_s = scaler.transform(X_train_full)\n",
    "X_test_s       = scaler.transform(X_test)\n",
    "\n",
    "# split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full_s, y_train_full_enc, test_size=0.2, random_state=42, stratify=y_train_full_enc\n",
    ")\n",
    "\n",
    "# save for use in models 1 & 2\n",
    "np.save(f\"{OUT_DIR}/X_train.npy\", X_train)\n",
    "np.save(f\"{OUT_DIR}/y_train.npy\", y_train)\n",
    "np.save(f\"{OUT_DIR}/X_val.npy\",   X_val)\n",
    "np.save(f\"{OUT_DIR}/y_val.npy\",   y_val)\n",
    "np.save(f\"{OUT_DIR}/X_test.npy\",  X_test_s)\n",
    "np.save(f\"{OUT_DIR}/y_test.npy\",  y_test_enc)\n",
    "\n",
    "# save for possible verification\n",
    "pd.DataFrame(X_train).to_csv(f\"{OUT_DIR}/X_train.csv\", index=False)\n",
    "pd.DataFrame(X_val).to_csv(f\"{OUT_DIR}/X_val.csv\", index=False)\n",
    "pd.DataFrame(X_test_s).to_csv(f\"{OUT_DIR}/X_test.csv\", index=False)\n",
    "\n",
    "print(\"Finished processing data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4b9b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
